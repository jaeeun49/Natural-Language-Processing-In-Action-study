{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처: http://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BERT\n",
    "<br>\n",
    "\n",
    "**'BERT: 언어 이해를 위한 양방향 트랜스포머 사전 학습\n",
    "(BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding)'**\n",
    "\n",
    "특정과제를 수행하기 위한 모델의 성능은, 데이터가 충분히 많다면 Embedding이 큰 영향을 미친다. 단어의 의미를 잘 표현하는 벡터로 embedding된 단어들이 훈련과정에서 당연히 좋은 성능을 낼 것이다. bert는 특정 과제를 하기 전에 사전 훈련된 Embedding을 통해 특정 과제의 성능을 더 좋게 할 수 있는 언어모델이라고 할 수 있다. \n",
    "\n",
    "**BERT는 다음과 같은 NLP의 여러 아이디어에 의해 만들어 졌다.**\n",
    "1. Semi-supervised Sequence Learning\n",
    "2. ELMo\n",
    "3. ULMFiT\n",
    "4. OpenAI transformer\n",
    "5. Transformer\n",
    "\n",
    "<img src=\"./image/bert1.jpg\" width=\"800\" height=\"500\">\n",
    "\n",
    "\n",
    "위의 그림을 보면 \n",
    "\n",
    "1. bert는 방대한 양의 Corpus(위키피디아, 웹문서, 책정보 등)를 이미 트레이닝시킨 언어 처리 모델이다. \n",
    "<br><br>\n",
    "\n",
    "# 2. Model Architecture\n",
    "<br>\n",
    "\n",
    "우선 버트는 트랜스포머의 인코더만을 사용한다. \n",
    "\n",
    "### **\"Model Input\"**\n",
    "\n",
    "<img src=\"./image/bert2.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "첫번째 입력 토큰은 CLS토큰과 함께 제공된다. 트랜스포머의 인코더 처럼 각각의 인코더 층은 셀프 어텐션을 수행하고, 이 결과를 피드-포워드 네트워크에 전달하고 결과를 다음 인코더 층으로 전달한다. 여기까지는 트랜스포머와 동일하지만 다음 output에서 다른점을 볼 수 있다. \n",
    "\n",
    "<img src=\"./image/bert3.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "### **\"Model Output\"**\n",
    "\n",
    "<img src=\"./image/bert4.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "입력 문장중에서 첫번째 위치의 결과에만 집중한다. 그 벡터는 우리가 선택한 분류기의 input으로 사용되며 논문은 단지 단층의 신경망을 분류기로 사용할때도 높은 성능을 보여주고 있다. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "# 3. A New Age of Embedding\n",
    "<br>\n",
    "\n",
    "### ELMo: Context Matters\n",
    "\n",
    "지금까지 word embedding으로 word2vec,glove를 주로 사용했고, 이는 단어 의미간 유사성, 문법간의 유사성 등을 포착할 수 있는 단어 벡터로 표현했다. 하지만 한 단어는 문맥에 따라 여러가지 의미를 가질 수 있지만 이러한 단어 임베딩 방법은 단어 하나당 단어 벡터가 하나만 나온다는 문제가 있다. 그래서 이러한 문제를 보완하고자 contextualized word-embeddings가 등장했다. \n",
    "\n",
    "<img src=\"./image/elmo.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "**엘모는 이처럼 단어에 대한 고정적 단어 벡터를 제공하는 대신에 문장을 보고서 문맥에 따른 단어 벡터값을 제공해준다.** 어떻게 이게 가능할까? \n",
    "\n",
    "<img src=\"./image/elmo22.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "엘모는 특정 단어에서 다음 단어를 예측하는 훈련을 통해 언어를 이해하고, 한단계 더 나아가 양방향 LSTM을 훈련시켜 언어 모델이 다음 단어 뿐만 아니라 이전 단어에 대한 감각도 갖게 한다. \n",
    "\n",
    "<img src=\"./image/elmo3.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "엘모는 은닉층을 그룹화하여 문맥을 고려하는 임베딩을 제공한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 4. ULM-FIT: Nailing down Transfer Learning in NLP\n",
    "\n",
    "ULM-Fit은 임베딩이상으로 사전 훈련 동안 모델이 학습한 것을 효과적으로 이용하는 방법을 소개했다. ULM-Fit은 다양한 과제에 대해 그 언어 모델을 효과적으로 fine-tune하는 방법을 소개했다. 마침내 NLP는 컴퓨터 비전이 할 수 있는 만큼의 전이 학습을 하는 방법을 가지게 되었다\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# 5. The Transformer: Going beyond LSTMs\n",
    "\n",
    "트랜스포머는 LSTM보다 long-term depence를 더 잘 다룬다고 알려져 있다. 트랜스포머의 인코더-디코더 구조는 기계 번역에 최적화되어있다. 하지만 문장 분류의 경우에는 어덯게 사용할 수 있을까? 다른 과제에 대해 fine-tune 될 수 있는 사전 훈련된 언어 모델에는 어떻게 사용할 수 있을까? \n",
    "\n",
    "<br><br>\n",
    "# 6. OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling\n",
    "\n",
    "NLP에 전이학습과 미세조정 언어 모델을 채택하기 위해서는 트랜스포머의 디코더만을 이용해도 된다. 디코더는 미래 토큰을 마스킹하기때문에 다음 단어를 예측하는 언어 모델의 자연스러운 선택이기 때문이다. \n",
    "\n",
    "<img src=\"./image/openai.jpg\" width=\"400\" height=\"300\">\n",
    "\n",
    "인코더가 없기 때문에 기존 트랜스포머가 가지는 인코더-디코더 어텐션은 없지만, 셀프 어텐션은 여전히 가지고 있다. 이러한 구조로 우리는 같은 언어 모델 과제에 대해서는 그 모델을 훈련시킬 수 있다.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "# 7. Transfer Learning to Downstream Tasks\n",
    "\n",
    "openai-transformer는 사전훈련 되었으며, 각 층은 언어를 합리적으로 처리하도록 조정되었으므로 우리는 그것을 downstream 업무에 사용하면 된다. 우선 메세지를 스팸인지 아닌지 분류하는 문제를 봐보자. \n",
    "\n",
    "<img src=\"./image/openai3.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "openai 논문은 다양한 과제의 input을 다루기 위한 많은 input 변형을 설명한다. 위의 이미지는 다양한 과제에 따른 모델 구조와 input변형을 보여주고 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 8. BERT : From Decoders to Encoders\n",
    "\n",
    "openai transformer는 트랜스포머를 기반으로 미세 조정 가능한 사전 훈련 모델을 제공했다. 이때, elmo의 언어모델을 양방향이었지만, openai 트랜스포머는 단방향 언어 모델을 훈련한다. **앞뒤를 모두 볼 수 있는 트랜스포머 기반의 언어모델을 만들 수 없을까?!**\n",
    "\n",
    "## Masked Language Model - Masked LM\n",
    "\n",
    "<img src=\"./image/bert5.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "**버트의 언어 모델은 input의 단어중 15%를 마스킹하고 모델에게 없어진 단어를 예측하라고 한다.** 논문에 따르면 80%는 MASK로 마스킹을 하고 10%는 다른 단어로 치환하고 10%는 그대로 둔다. \n",
    "\n",
    "<br>\n",
    "\n",
    "## Two-sentence Tasks - Next Sentence Prediction(NSP)\n",
    "\n",
    "버트가 여러 문장 간의 관계를 더 잘 다루도록 하기 위해 사전 훈련 과정에 추가 작업이 포함되어 있다. 두 문장(A,B)이 주어졌을 때, B가 A뒤에 오는 문장일 가능성이 있는가?\n",
    "\n",
    "<img src=\"./image/bert6.jpg\" width=\"600\" height=\"500\">\n",
    "\n",
    "**버트가 사전 훈련한 두번째 업무는 2문장 분류 작업이다.** \n",
    "\n",
    "<br>\n",
    "\n",
    "## Task specific-Models\n",
    "\n",
    "Bert 논문은 다양한 문제로 버트를 사용하는 방법을 다음과 같이 보여준다. 한개 문장으로 적용부를 구성할 수도 이쏙, text 페어(예를 들면 질의응답)로 구성할 수도 있다. 꼭 머신러닝 기법이 아니더라도, 기존 검색 기법이나 다른 알고리즘 태스트로 하이퍼 파라미터를 튜닝시킬수도 있다.  \n",
    "<img src=\"./image/bert7.jpg\" width=\"550\" height=\"400\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## BERT for feature extraction\n",
    "\n",
    "버트는 미세 조정 접근뿐만 아니라 엘모처럼 문맥을 고려하는 워드 임베딩을 만들기 위해서도 사용할 수 있다. \n",
    "\n",
    "<img src=\"./image/bert8.jpg\" width=\"550\" height=\"400\">\n",
    "\n",
    "각각의 토큰이 지나가는 인코더 층마다의 결과물이 그 토큰을 표현하는 피쳐로 사용될 수 있다. 그럼 저 많은 단어 벡터중 어떤것이 문맥을 잘 고려한 임베딩일까? 논문은 6가지 선택을 가지고 시험하였다. \n",
    "\n",
    "<img src=\"./image/bert9.jpg\" width=\"550\" height=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
