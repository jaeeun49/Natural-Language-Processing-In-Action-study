{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 퍼셉트론\n",
    "\n",
    "<br><br>\n",
    "컴퓨터의 계산 능력과 저장용량이 폭발적으로 커지면서 50년대 프랭크 로젠블랫이 처음 제안한 퍼셉트론이 다시 전성기를 맞았다. 기본적으로 신경망을 사용한다는 것은 한 견본sample을 신경망 알고리즘에 입력해서 알고리즘이 그 견본에 관해 어떤 값을 산출하는지 보는 것이다. 예를 들어 이진 분류의 경우 신경망 알고리즘은 주어진 견본이 특정 부류에 속하는지 아닌지 여부를 출력한다. 이런식으로 **신경망을 활용하려면 먼저 하나의 견본이 어떤 특징들로 구성되는지를 사람이 결정해야 하며, 적절한 특징들을 선택하는 것이 기계학습 시스템 개발에서 가장 어려운 과제 중 하나이다.** Iris 데이터를 이용해 어떤 품종을 예측하는 경우에는 **꽃잎 길이, 꽃잎 너비** 등이 특징이 될것이고, 로젠블랫이 했던 이미지를 컴퓨터에게 식별하도록 학습시키는 경우, **이미지 작은 영역의 빛의 세기 즉 픽셀**이 특징이 될것이다. 그 다음으로 이 특징들 각각에 가중치를 부여해야 한다. **각 특징과 해당 가중치의 곱을 모두 합한것이 퍼셉트론에 들어오는 입력값이다.** 퍼셉트론이 발화하려면 이 값이 특정 문턱값보다 커야 한다. 그러면 뉴런은 1을 출력하고, 그렇지 않으면 0을 출력한다. 이처럼 **입력에 기초해서 뉴런의 출력을 결정하는 함수를 활성화 함수라고 부른다.** 밑의 그림은 활성화 함수로 계단 함수를 가지는 퍼셉트론을 도식화한것이다.\n",
    "<img src=\"./image/perceptron.jpg\" width=\"500\" height=\"200\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론의 학습\n",
    "\n",
    "<br><br>\n",
    "신경망의 핵심은 가중치들을 신경망이 스스로 학습하게 하는 것이다. 퍼셉트론은 **주어진 입력에 대한 추측이 맞았는지 틀렸는지(얼마나 틀렸는지) 에 대한 함수에 기초해서 가중치들을 증가하거나 감소함으로써 학습한다.** 그런데 학습을 시작하려면 가중치들의 초깃값을 정해야하는데 일반적으로 정규분포에서 추출한 0에 가까운 난수들을 사용한다. 이후 학습 과정에서 서로 다른 여러 견본이 신경망 시스템에 입력되며, 그럴 때마다 뉴런의 출력이 정답인지 아닌지에 기초해서 가중치들이 조금씩 갱신된다. 이 과정의 (신경망 학습 전체의) 핵심은 각 가중치를 해당 특징이 오차에 기여하는 정도에 근거해서 갱신한다는 것이다.  하나의 예시를 들어 퍼셉트론이 가중치를 갱신하는 방식을 이해하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 논리합 문제의 설정\n",
    "\n",
    "sample_data = [[0,0], # 거짓,거짓\n",
    "              [0,1],  # 거짓, 참\n",
    "              [1,0],  # 참, 거짓\n",
    "              [1,1]]  # 참, 참\n",
    "expected_result = [0, # 거짓 or 거짓은 거짓\n",
    "                   1, # 거짓 or 참  은 참\n",
    "                   1, # 참   or 거짓은 참\n",
    "                   1] # 참   or 참  은 참\n",
    "activation_threshold = 0.5 # 퍼셉트론의 입력값이 0.5가 넘으면 1(참)으로 출력하도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 correct answers out of 4, for iteration 0\n",
      "2 correct answers out of 4, for iteration 1\n",
      "3 correct answers out of 4, for iteration 2\n",
      "4 correct answers out of 4, for iteration 3\n",
      "4 correct answers out of 4, for iteration 4\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과에 따라 가중치들을 갱신하는 과정 반복\n",
    "# 초기 가중치 설정\n",
    "import numpy as np\n",
    "from random import random\n",
    "weights = np.random.random(2)/1000            # 0보다 크고 .001보다 작은 무작위 소수점을 가중치로 설정\n",
    "# 치우침 가중치도 설정\n",
    "bias_weight = np.random.random()/1000\n",
    "\n",
    "\n",
    "# 퍼셉트론 학습\n",
    "for iteration_num in range(5):\n",
    "    correct_answer = 0\n",
    "    for idx, sample in enumerate(sample_data):\n",
    "        input_vector = np.array(sample)        # np.array를 통해 [0, 0] 처럼 벡터로 표현됨\n",
    "        weights = np.array(weights)\n",
    "        activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
    "        if activation_level > activation_threshold:\n",
    "            perception_output = 1\n",
    "        else:\n",
    "            perception_output = 0\n",
    "            \n",
    "        if perception_output == expected_result[idx]:\n",
    "            correct_answer += 1\n",
    "            \n",
    "        # 다음이 가중치를 갱신하는 과정인데 현재 입력 견본의 각 특징 크기에 비례해서 해당 가중치를 갱신한다. \n",
    "        # 입력 특징이 작거나 0이면 오차의 크기와는 무관하게 가중치가 조금만 변한다.\n",
    "        # 반대로 입력 특징이 크면 가중치도 크게 변한다. \n",
    "        new_weights = []\n",
    "        for i, x in enumerate(sample):\n",
    "            new_weights.append(weights[i] + (expected_result[idx] - perception_output)*x)\n",
    "        # 다음의 치우침 가중치도 다른 입력 특징 가중치들과 같은 방식으로 갱신한다\n",
    "        bias_weight = bias_weight + ((expected_result[idx] - perception_output)*1)\n",
    "        weights = np.array(new_weights)\n",
    "    print('{} correct answers out of 4, for iteration {}'\n",
    "         .format(correct_answer, iteration_num))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 1]\n",
    "weight = [0.5, 0.5]\n",
    "np.dot(a,weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론의 약점\n",
    "<br><br>\n",
    "퍼셉트론은 자료 집합의 특징들과 목표변수 사이의 관계를 서술하는 하나의 직선 방정식을 찾는다. 즉, 퍼셉트론이 하는 일은 선형회귀이다. 이러한 퍼셉트론은 비선형 방정식이나 비선형 관계를 배우지 못한다. 민스키와 패퍼트는 이러한 퍼셉트론의 결점을 증명하였고 이후 인공지능은 첫 번째 겨울을 맞이하게 된다. 하지만 민스키 등의 책이 나온지 17년 후, 루멜하트와 매클리랜드의 공동연구는 **퍼셉트론을 여러개 함께 사용하면 비선형 관계를 가진 XOR문제를 풀 수 있음을 보였다.** 루멜하트와 매클리랜드는 여러 개의 퍼셉트론을 함께 사용하고 **오차를 각 퍼셉트론에 적절한 비율로 배분함으로써** 기존 퍼셉트론의 한계를 극복했다. 이를 위해 그들이 사용한 것이 바로 현재 기계 학습의 핵심인 **역전파**이다. 즉, 여러 층의 뉴런들 사이에서 오차를 역전파해서 가중치를 갱신한다는 그들의 착안에서 최초의 현대적인 신경망이 탄생했다.  \n",
    "\n",
    "<br>\n",
    "하지만 코드 한줄로 풀 수 있는 XOR문제를 푸는데 상당한 역전파 계산이 필요하다는 것은 컴퓨터 계산 능력의 낭비로 간주되었다. 일상적인 용도로 사용할 수 없음이 판명되면서 신경망은 다시 후미진 세계로 물러났다. 약 1990년에서 2010년까지의 이 시기를 인공지능의 두 번째 겨울이라고 부른다. 그러나 시간이 지나 컴퓨터의 계산 능력이 급격히 증가하고 사용가능한 원본 자료도 넘쳐나면서 알고리즘의 계산 비용과 자료 집합의 제한이 더 이상 신경망의 발목을 잡지 않게 되면서 2010년대 초중반에 신경망이 세번째로 부활했다. \n",
    "\n",
    "### 퍼셉트론의 두 번째 부흥기 \n",
    "<br><br>\n",
    "루멜하트등은 **하나 (또는 여러 개)의 퍼셉트론에 입력을 넣고, 그 퍼셉트론의 출력을 다른 퍼셉트론의 입력에 연결해서 하나의 네트워크를 형성하는 방법**을 발견했다. 그 네트워크의 끝에 있는 퍼셉트론의 출력이 전체 신경망의 예측 결과이다. 이러한 착안을 실현하는데 있어 핵심은 **출력층 이전의 층들에 있는 가중치들을 어떻게 갱신할 것**인가이다. 앞에서 우리는 예측 결과의 오차에 기초해서 퍼셉트론의 가중치를 갱신하는 방법을 배웠다. 그러한 오차를 측정하는 함수를 **비용함수** 또는 **손실함수**라고 부른다. **비용함수는 신경망에 입력된 질문 즉, 입력견본 x에 대해 신경망이 출력해야 하는 정답과 신경망이 실제로 출력한 예측값 y차이를 수량화하는 함수이다.** 퍼셉트론 훈련의 목표는 모든 가능한 입력 표본에 대해 이 비용함수를 최소화하는 것이다.대부분의 신경망 프레임워크에는 최상의 비용함수가 이미 결정되어 있다. 중요한것은 **자료 집합 전체에 대해 비용함수를 최소화하는 것이 신경망 훈련의 궁극적인 목표라는 것이다.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 역전파\n",
    "<br><br>\n",
    "다수의 퍼셉트론을 함께 사용한다면 비선형 함수들도 선형 함수들처럼 퍼셉트론들로 근사할 수 있음을 보였다. 그럼 여러 개의 퍼셉트론의 가중치들을 어떻게 갱신해야 할까? 오차를 가중치들에 배분한다는 것이 무슨뜻일까? 두 퍼셉트론을 직렬로 연결해서 한 퍼셉트론의 출력이 다른 퍼셉트론에 입력되게 하면 흥미로운 일이 생긴다. 다중 퍼셉트론에서 가중치들은 그것이 전체 오차에 얼마나 기여하느갸에 기초해서 갱신된다. 그런데 한 퍼셉트론의 출력이 다른 퍼셉트론의 입력이 되면 둘째 퍼셉트론의 관점에서 보는 오차의 정의가 명확하지 않다. 이 문제를 해결하려면 밑의 그림에서 보듯 한 층의 한 가중치(W1i)가 오차에 기여하는 정도를 그다음 층에 있는 다른 가중치(W1j,W2j)들에 근거해서 계산하는 방법이 필요하다. 그리고 역전파가 바로 그러한 방법이다. \n",
    "<img src=\"./image/역전파.jpg\" width=\"500\" height=\"200\">\n",
    "신경망의 각 연결에는 하나의 가중치가 부여된다. 신경망의 둘째 층의 뉴런들에 부여된 가중치는 원래의 입력 벡터가 아니라 그 이전 층의 출력에 적용되는 것이다. 이 때문에 첫 층이 전체 오차에 기여한 양을 둘째 층의 입력에 영향을 미침으로써 오차에 간접적으로 영향을 미친다. \n",
    "\n",
    "'오차의 역전파'를 줄인 역전파 알고리즘은 신경망의 입력과 출력, 그리고 바람직한 목푯값이 주어졌을 때 특정 가중치의 적절한 변화량(갱신량)을 구하는 알고리즘이다. 이전에는 계단함수를 뉴런의 활성화 함수로 사용했지만 역전파를 위해서는 비선형 연속 미분가능 함수를 활성화 함수로 사용해야 한다. 이런 활성화 함수를 사용하는 뉴런을 두 값 사이의 (이를테면 0과 1사이의) 연속값을 출력한다. 활성화 함수로 흔히 쓰이는 S자형 함수(시그모이드 함수)가 그런 조건을 만족하는 함수이다. 왜 활성화 함수가 미분가능해야할까? 어떤 함수의 미분을 계산할 수 있다면, 그 함수 자체의 여러 변수에 대한 함수의 편미분도 계산할 수 있다. 신경망 마법의 힌트는 **여러 변수에 대한 함수의 편미분들**이라는 문구이다.\n",
    "\n",
    "### 미분\n",
    "<br><br>\n",
    "한 뉴런의 출력이 그 다음 뉴런의 입력이라는 것은 **한 뉴런이 계산하는 함수의 결과값을 그 다음 뉴런이 계산하는 함수의 인수로 사용**한다는 뜻이다. **즉 신경망 전체가 하나의 합성 함수**임을 알 수 있다. 그리고 합성 함수는 미분의 연쇄 법칙으로 미분할 수 있다. 즉 뉴런에 주어진 입력에 대한 뉴런의 활성화 함수의 미분을 구할 수 있다. 그리고 그 미분이 있으면 **해당 뉴런이 최종 오차에 얼마나 기여했는지 계산해서 그에 따라 가중치를 적절히 갱신할 수 있다.** 공식을 보면 i번째 층과 j번째 층을 연결하는 가중치의 변화량은 **그 가중치에 대한 오차의 편미분**에 학습속도 알파를 곱한 것임을 말해 준다. 이상의 공식들에 기초한 훈련 과정을 간략히 설명하면 이렇다. \n",
    "- 신경망에 훈련 자료의 한 견본을 입력한다. \n",
    "- 그 입력이 순전파를 통해서 신경망의 출력층에 도달해서 출력값이 나오면, 그것으로 오차를 계산한다. \n",
    "- 이제 그 오차를 출력층에서 입력층 쪽으로 역전파하면서 각 층의 각 가중치를 앞의 공식들을 이용해서 갱신한다. \n",
    "- 이 과정을 훈련자료의 모든 견본에 대해 반복한다. \n",
    "- 모든 입력에 대해 가중치들을 갱신하는 훈련주기를 하나의 epoch이라고 부른다. \n",
    "\n",
    "위의 공식에서 알파는 학습속도이다. 즉, 이 매개변수는 주어진 훈련 주기 또는 batch에서 가중치를 관측된 오차에 기초해 얼마나 보정할 것인지를 결정한다. 알파가 너무 크면 가중치가 너무 크게 변해서 최적해를 건너뛰고, 그 다음 갱신에서는 가중치가 그 반대 방향으로 이전보다도 더 크게 변해서 최적해에서 더욱 멀어지는 현상이 발생할 수 있다. 반대로 알파가 너무 작으면 수렴에 도달하는 시간이 비현실적으로 길어질 수 있다. \n",
    "\n",
    "### 오차곡선\n",
    "<br><br>\n",
    "반복적으로 말하지만 신경망 훈련의 목표는 비용함수를 최소화하는 것, 비용함수가 최소가 되는 최적의 가중치들을 구하는 것이다. 모든 가중치 조합에 대해 오차값(입력 견본과 그 목푯값의 차이를 제곱한것)을 그래프로 그리면 다차원 곡면이 생긴다. 이 곡면에서 가장 낮은 점은 오차의 최솟값에 해당하고 이를 최소점이라고 부른다. 이 최소점의 좌표성분들은 주어진 훈련견본에 대해 최적의 출력을 산출하는 가중치들이다. 이처럼 한 입력 견본에 관한 오차 곡면과 비슷하게, 훈련 집합 전체에 대한 이러한 오차 곡면의 최소점의 좌표성분들은 주어진 모형이 훈련 집합 전체에 대해 가장 좋은 성과를 내게 하는 가중치들이다. \n",
    "<img src='./image/오차곡면.jpg' width='500' height='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 케라스: 신경망 구현\n",
    "<br>\n",
    "pyTorch나 Theano, TesorFlow, Lasagne 처럼 파이썬의 느린 속도를 극복할 수 있는 기계학습 라이브러리가 많이있다. 여기서는 Keras를 사용할것이다.케라스를 이용해 간단한 XOR문제를 푸는 신경망을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential         # -> 기본 케라스 모형 클래스\n",
    "from tensorflow.keras.layers import Dense, Activation  # -> Dense는 완전 연결 신경망에 해당한다.\n",
    "from tensorflow.keras.optimizers import SGD            # -> sgd는 확률적 경사 하강법 알고리즘을 구현한다. \n",
    "\n",
    "# x_train is sample data\n",
    "# y_train the expected outcome for example\n",
    "x_train = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "y_train = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "num_neurons = 10\n",
    "model.add(Dense(num_neurons, input_dim=2))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "# 출력증 레이어 \n",
    "model.add((Dense(1)))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary는 각 층의 형태와 가중치 개수(Param)을 보여준다. 첫 층은 뉴런이 10개이고 뉴런당 가중치는 세개(입력의 두 특징에 대한 것 두개와 치우침 가중치 하나)이다. 따라서 첫 층이 학습해야 할 가중치는 총 30개이다. 그 다음 층인 출력층은 뉴런이 하나뿐이고, 이 뉴런에 첫 층의 뉴런10개가 연결된다. 거기에 치우침 항이 하나 있으므로 가중치는 총 11개이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD는 케라스의 다양한 모형 최적화 객체중 하나인 확률적 경사 하강법 최적화 객체를 돌려준다. 이 최적화 객체는 확률적 경사 하강법을 이용해서 손실을 최소화 함으로써 학습모형을 최적화 한다. 매개변수 lr는 각 가중치에 대한 오차의 미분을 가중치 갱신에 적용하는 비율로, 학습속도 알파에 해당한다. 둘째 행에서는 이 최적화 객체를 지정해서 모형을 컴파일한다. compile메서드는 주어진 인수들로 모형을 구성(컴파일)할 뿐 실제로 훈련하지는 않는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 2s 377ms/sample - loss: 0.7091 - accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 923us/sample - loss: 0.7079 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 478us/sample - loss: 0.7068 - accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7058 - accuracy: 0.2500\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7049 - accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.7040 - accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 719us/sample - loss: 0.7031 - accuracy: 0.2500\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.7023 - accuracy: 0.2500\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.7015 - accuracy: 0.2500\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.7008 - accuracy: 0.2500\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7000 - accuracy: 0.2500\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6993 - accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.6986 - accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6979 - accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6972 - accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6965 - accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 773us/sample - loss: 0.6958 - accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6951 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6937 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6931 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6924 - accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 740us/sample - loss: 0.6917 - accuracy: 0.7500\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 978us/sample - loss: 0.6911 - accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 490us/sample - loss: 0.6904 - accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.6898 - accuracy: 0.7500\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 747us/sample - loss: 0.6891 - accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 757us/sample - loss: 0.6885 - accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.6878 - accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6872 - accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 762us/sample - loss: 0.6865 - accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6859 - accuracy: 0.7500\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6853 - accuracy: 0.7500\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6846 - accuracy: 0.7500\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6840 - accuracy: 0.7500\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6833 - accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 931us/sample - loss: 0.6827 - accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 627us/sample - loss: 0.6821 - accuracy: 0.7500\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 1000us/sample - loss: 0.6814 - accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 750us/sample - loss: 0.6808 - accuracy: 0.7500\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 658us/sample - loss: 0.6802 - accuracy: 0.7500\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 739us/sample - loss: 0.6795 - accuracy: 0.7500\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6789 - accuracy: 0.7500\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 735us/sample - loss: 0.6782 - accuracy: 0.7500\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 735us/sample - loss: 0.6776 - accuracy: 0.7500\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 744us/sample - loss: 0.6770 - accuracy: 0.7500\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6763 - accuracy: 0.7500\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 706us/sample - loss: 0.6757 - accuracy: 0.7500\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6750 - accuracy: 0.7500\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 756us/sample - loss: 0.6744 - accuracy: 0.7500\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6738 - accuracy: 0.7500\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6731 - accuracy: 0.7500\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 735us/sample - loss: 0.6725 - accuracy: 0.7500\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6718 - accuracy: 0.7500\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6712 - accuracy: 0.7500\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6705 - accuracy: 0.7500\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6699 - accuracy: 0.7500\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6692 - accuracy: 0.7500\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 952us/sample - loss: 0.6685 - accuracy: 0.7500\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6679 - accuracy: 0.7500\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6672 - accuracy: 0.7500\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6666 - accuracy: 0.7500\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6659 - accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6652 - accuracy: 0.7500\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6645 - accuracy: 0.7500\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6639 - accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6632 - accuracy: 0.7500\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6625 - accuracy: 0.7500\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.6618 - accuracy: 0.7500\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6612 - accuracy: 0.7500\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6605 - accuracy: 0.7500\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6598 - accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6591 - accuracy: 0.7500\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 793us/sample - loss: 0.6584 - accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 754us/sample - loss: 0.6577 - accuracy: 0.7500\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6570 - accuracy: 0.7500\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 720us/sample - loss: 0.6563 - accuracy: 0.7500\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 756us/sample - loss: 0.6556 - accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 498us/sample - loss: 0.6549 - accuracy: 0.7500\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 615us/sample - loss: 0.6541 - accuracy: 0.7500\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6534 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.6527 - accuracy: 0.7500\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 727us/sample - loss: 0.6520 - accuracy: 0.7500\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6512 - accuracy: 0.7500\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6505 - accuracy: 0.7500\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6498 - accuracy: 0.7500\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6490 - accuracy: 0.7500\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6483 - accuracy: 0.7500\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6475 - accuracy: 0.7500\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6468 - accuracy: 0.7500\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6460 - accuracy: 0.7500\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6453 - accuracy: 0.7500\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6445 - accuracy: 0.7500\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6438 - accuracy: 0.7500\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 999us/sample - loss: 0.6430 - accuracy: 0.7500\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6422 - accuracy: 0.7500\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6414 - accuracy: 0.7500\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6406 - accuracy: 0.7500\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6399 - accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 978us/sample - loss: 0.6391 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23499559348>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we save the structure and learned weights for later use\n",
    "model_structure = model.to_json()\n",
    "# -> 나중에 케라스의 보조 메서드로 신경망의 구조를 다시 불러올 수 있도록 신경망이 구조를 JSON형식으로 저장한다. \n",
    "with open(\"basic_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"basic_weights.h5\")\n",
    "# -> 훈련된 가중치들은 반드시 따로 저장해야한다. 앞에서는 신경망의 구조만 저장한것이다. \n",
    "# 이후 신경망 구조로 신경망 인스턴스를 다시 생성한후 이 가중치들을 그 인스턴스에 적해하면 된다. \n",
    "# 케라스는 이러한 파일들로부터 신경망을 다시 생성하고 가중치들을 불러오는 메서드들을 제공한다. \n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[[0.4445804 ]\n",
      " [0.5267706 ]\n",
      " [0.52401716]\n",
      " [0.51383156]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_classes(x_train))\n",
    "print(model.predict(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 깊이 공부할 것\n",
    "- 다양한 활성화 함수들의 장단점\n",
    "- 오차의 반영 정도를 결정하는 학습속도를 잘 선택하는 방법\n",
    "- 드롭아웃기법. 주어진 한 훈련 패스에서 가중치 중 일부를 무작위로 선택해서 탈락시킴으로써 모형이 훈련 집합에 괘대적합하는 일을 방지한다.\n",
    "- 한 가중치가 다른 가중치보다 너ㅜㅁ 많이 커지거나 작아지지 않도록 인위적으로 제한을 가하는 정칙(regularization)기법들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약\n",
    "- 학습은 비용함수를 최소화하는 방향으로 일어난다.\n",
    "- 신경망 학습의 비결은 역전파 알고리즘이다.\n",
    "- 가중치가 모형의 오차에 기여하는 정도는 학습을 위해 가중치를 변화하는 정도와 직접 연관된다.\n",
    "- 신경망은 본질적으로 하나의 최적화 엔진이다. \n",
    "- 훈련 시에는 오차 곡면상의 구덩이 (극소점)를 조심해야 한다. 오차가 꾸준히 줄어들지 않는다면 뭔가 문제라 있는 것이다. \n",
    "- 케라스틑 신경망을 위한 수학 연산을 좀 더 손쉽게 사용할 수 있게 해주는 편리한 라이브러리이다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
