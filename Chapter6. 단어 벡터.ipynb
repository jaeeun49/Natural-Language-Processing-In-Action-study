{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "\n",
    "<br><br>\n",
    "**단어 벡터란 단어의 의미를 나타내는 수치 벡터로, 용어 빈도에 기초한 표현들은 정수 희소 벡터였지만, 단어벡터는 실수 밀집(빈 성분이 없는 )벡터이다. 이 밀집 벡터를 단어의 의미에 관한 질의와 논리적 추론에 사용할 수 있다.**\n",
    "\n",
    "문서나 문장의 모든 단어가 아니라 한 단어의 이웃단어 몇개만으로 작은 단어 모음을 만든다. 한 단어의 이웃들이 그 단어의 의미에 미치는 영향과 그런 관계들이 문장의 전체 의미에 미치는 영향을 파악할 수 있다. 단어 벡터를 이용하면 동의어나 반의어를 식별할 수 있고, 더 나아가 같은 범주에 속한 단어들도 식별할 수 있다. \n",
    "\n",
    "#  1. word2vec\n",
    "\n",
    "2012년 마이크로소프트 수습사원인 토마시 니콜로프는 어떤 단어가 목표 단어 근처에 출현할 가능성을 예측하도록 신경망을 훈련했다. 이후 구글에 입사해 2013년에 동료들과 함께 단어 벡터들을 생성하는 소프트웨어를 공개했고 그 소프트웨어가 word2vec이다. \n",
    "\n",
    "word2vec은 축구가 스포츠중 하나라는 사실을 라벨링으로 알려줄 필요 없이, 그냥 텍스트를 읽어서 스스로 학습한다. word2vec의 강력함은 이런 비지도 학습 능력에서 비롯한다. 실제로는 범주화되지 않고 분류명이 붙어 있지 않은 비구조적 자연어 텍스트 구조로 가득하고 그것들을 그대로 word2vec의 학습에 사용할 수 있다. \n",
    "\n",
    "정리하자면 word2vec을 이용하면 토큰 출현 횟수와 빈도들로 이루어진 자연어 벡터들을 그보다 훨씬 낮은 차원의 단어 벡터 공간으로 변환할 수 있다 .그리고 그러한 저차원 공간에서 벡터들에 통상적인 선형대수 벡터 연산들을 적용해서 새로운 벡터를 얻고, 그 벡터를 다시 자연어 공간으로 변환함으로써 자연어 추론 문제를 풀 수 있다. -> **벡터 지향적 추론**\n",
    "\n",
    "## 1) 벡터 표현을 계산하는 방법\n",
    "<br>\n",
    " 어떻게 단어의 의미를 말해주는 수치들로 구성된 단어 벡터를 만들 수 있을까? 두가지 방법으로 word2vec을 훈련하는 과정을 살펴볼것이다. 하지만 단어 벡터 표현 계산을 자원 소비가 심하기 때문에 대부분의 응용에서는 **Glove** 나 **fastText** 와 같은 미리 훈련된 단어 벡터 모형을 사용할 것이다. \n",
    "\n",
    "### 스킵그램 - Skip Gram\n",
    "- 주어진 목표 단어의 문맥을 이루는 주변 단어들을 예측한다. 즉, **한 단어가 입력이고 그 주변 단어들이 출력이다. \n",
    "- 주어진 입력 단어에 기초해서 일정 범위 이내의 주변 단어들을 예측하도록 모형을 훈련. \n",
    "- 스킵그램이란 중간에 빈자리가 있는 n-그램을 말한다. \n",
    "<img src=\"./image/skipgram.jpg\" width=\"500\" height=\"200\">\n",
    "- 처음 두 주변 단어에 대한 신경망 입력과 출력을 보여준다. 입력단어는 'Monet'이고 신경망이 예측할 단어는 'Claude'아니면 'painted'이다. \n",
    "- 지금처럼 주변 단어가 두개인 경우에는 각 주변 단어를 입력단어에 대한 기대 출력으로 삼아서 훈련과정을 두번 반복한다. \n",
    "- 출력층의 노드들은 소프트맥스 활성화 함수를 이용해 각 출력 단어가 주어진 입력 단어 주변에 등장할 확률을 계산한다. \n",
    "- 그런 다음 출력층은 확률이 가장 큰 단어에 해당하는 성분만 1이고, 나머지 성분들은 0인 하나의 원핫 벡터를 출력한다. \n",
    "- 이처럼!!! 주어진 입력에 대해 출력 원핫 벡터와 손실함수를 계산하고 그로부터 역전파를 진행해 연결 가중치들을 학습하는 과정이 모두 끝나면 **가중치들은 단어의 의미를 반영하게 되는 것이다.** \n",
    "- 가중치 행렬의 각 행은 어휘의 각 단어 주변에 나타날 가능성이 큰 단어들을 말해준다. 그리고 주변에 나타나는 단어들이 비슷한 두 단어는 그 의미나 쓰임새도 비슷할 것이라고 가정할 수 있다. \n",
    "<img src=\"./image/skipgram_neural2.jpg\" width=\"500\" height=\"200\">\n",
    "<img src=\"./image/skipgram_neural1.jpg\" width=\"500\" height=\"200\">\n",
    "\n",
    "\n",
    "### 연속 단어 모음 - CBOW (continuous bag of word)\n",
    "- 주어진 **주변 단어들(입력)으로 부터 하나의 목표 단어(출력)을 예측한다.** \n",
    "- 다수의 주변 단어들은 멀티핫 벡터로 표현하는데, 이 멀티핫 벡터는 해당 주변 단어들의 원핫 벡터들을 모두 합한 것이다. \n",
    "<img src=\"./image/cbow.jpg\" width=\"500\" height=\"200\">\n",
    "\n",
    "말뭉치가 작을 때, 자주 쓰이지 않는 단어들이 주어졌을 때 스킵그램이 잘 작동하며, 연속 단어 모음 접근 방식은 자주 쓰이는 단어들에 대해 정확도가 높으며, 훈련이 훨씬 빠르다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. gensim라이브러리 이용해서 word2vec 모형 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 전처리 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# word2vec 훈련을 제어하는 매개변수들\n",
    "num_features = 300   # 단어 벡터의 차원 수\n",
    "min_word_count = 3   # 모형에 포함시킬 단어의 최소 빈도, 말뭉치가 작다면 이 최소 빈도를 줄이고 말뭉치가 크다면 더 키우는 것이 바람직하다\n",
    "num_workers = 2      # 훈련에 사용할 CPU 코어 수.\n",
    "window_size = 6      # 문맥 구간 크기\n",
    "subsampling = 1e-3   # 고빈도 용어를 위한 부표집 문턱값.. 이게 뭐쥬?!\n",
    "\n",
    "# gensim의 word2vec모형은 각 문장이 토큰들의 리스트인 목록을 요구한다. \n",
    "# 이러한 문장 단위 처리는 단어 벡터들이 인접한 두 문장에 걸쳐 있는 단어들의 관계를 학습하지 못하게 하기 위한 것이다. \n",
    "# 다음은 word2vec을 위한 문장 목록의 예이다. \n",
    "token_list = [\n",
    "  ['to', 'provide', 'early', 'intervention/early', 'childhood', 'special',\n",
    "   'education', 'services', 'to', 'eligible', 'children', 'and', 'their',\n",
    "   'families'],\n",
    "  ['essential', 'job', 'functions'],\n",
    "  ['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to',\n",
    "   'complete', 'educational', 'assessments', 'for']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) word2vec 모형의 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(token_list, workers=num_workers, size=num_features, \n",
    "                 min_count=min_word_count, window=window_size, sample=subsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "궁극적으로 필요한 것은 **은닉증의 가중치 행렬**임을 기억하자. 일단 훈련을 마치고 단어 모형을 동결하면 불필요한 정보를 모두 폐기할 수 있으며 그러면 메모리 요구량이 절반이 된다. 다음은 신경망에서 불필요한 출력 가중치들을 폐기하는 명령이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)\n",
    "\n",
    "# init_sims 메서드는 은닉층의 가중치들을 저장하고, 단어 공동 출현 확률들을 예측하는 출력 가중치들을 폐기한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': <gensim.models.keyedvectors.Vocab at 0x159d2d69988>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련된 단어 벡터 모형을 저장해 두면 매번 모형을 훈련할 필요가 없다. \n",
    "\n",
    "model_name = '___word2vec_model'\n",
    "model.save(model_name)\n",
    "\n",
    "# 저장된 word2vec 모형의 적재 및 사용\n",
    "\n",
    "model = Word2Vec.load(model_name)\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 다른 Vectorization 기법\n",
    "\n",
    "### 1) word2vec & Glove\n",
    "\n",
    "<br><br>\n",
    "word2vec은 반드시 역전파를 이용해서 훈련해야 하는 신경망에 의존한다는 한계가 있다. 일반적으로 역전파는 경사 하강법을 이용해 비용함수를 직접 최적화하는 것보다 덜 효율적이다. \n",
    "\n",
    "스탠퍼드 대학교 NLP연구팀은 word2vec이 효과적인 이유를 파악하고 word2vec의 훈련에서 최적화되는 비용함수가 어떤 것인지를 밝히는 연구를 했다. 그들은 **공동 출현 횟수를 세어서 정방행렬 형태로 기록한 후 그 행렬에 SVD(특이값 분해)를 적용해서 word2vec으로 산출한 것과 동일한 두개의 가중치 행렬을 얻을 수 있었다.** 핵심은 공동 출현 행렬(co-occurrence matrix)을 동일한 방식으로 정규화 하는 것이었다. 연구팀은 이러한 직접적인 최적화 방법에 Glove라는 이름을 붙였는데, 이 이름은 이 기법이 말뭉치 전체에 대한 단어 공동 출현 빈도의 전역 벡터들을 최적화한다는 점에서 비롯되었다. \n",
    "\n",
    "Glove는 word2vec만큼이나 정확한 언어 모형을 좀 더 빠르게 산출할 수 있다. 단어 벡터를 이용한 의미 추론이라는 개념을 대중화한 것은 word2vec이지만, 지금 단어 벡터 모형을 훈련한다면 word2vec보다 Glove를 사용하는 것이 바람직하다. word2vec보다 Glove가 벡터 표현들의 전역 최적해를 찾아낼 가능성이 크다.\n",
    "Glove의 장점\n",
    "- 훈련이 빠르다\n",
    "- Ram과 Cpu 효율성이 좋다(더 큰 말뭉치를 처리할 수 있다.)\n",
    "- 자료를 좀 더 효율적으로 사용한다. (이는 말뭉치가 작을 때 도움이 된다)\n",
    "- 같은 훈련 자료로 훈련했을 때 word2vec보다 더 정확한 결과를 제공한다.\n",
    "\n",
    "### 2) fastText\n",
    "<br><br>\n",
    "페이스북의 연구자들은 word2vec의 개념에서 출발해 새로운 방식의 훈련 알고리즘을 고안했다. word2vec는 한 단어의 주변 단어들을 예측하지만, 페이스북이 제시한 fastText는 각 단어를 n문자 그램들로 분할해서 한 n문자 그램의 주변 n문자 그램들을 예측한다. \n",
    " \n",
    "예를 들어, whisper를 2문자 그램들과 3문자 그램들로 분할할 수 있다. -> wh,whi,hi,his,is,isp,sp,spe,pe,per,er\n",
    "fastText는 모든 n문자 그램에 대한 벡터 표현을 훈련하는데 '모든'n문자 그램에는 단어, 철자가 틀린단어, 개별 글자들도 포함된다. \n",
    "이 접근 방식의 장점은 드물게 쓰이는 단어들을 원래의 word2vec보다 훨씬 잘 처리한다는 것이다. \n",
    "\n",
    "### 3) Word2vec & LSA\n",
    "<br><br>\n",
    "LSA 장점\n",
    "- 훈련이 빠르다\n",
    "- 긴 문서들을 잘 분류한다.\n",
    "\n",
    "word2vec 과 Glove의 장점\n",
    "- 큰 말뭉치를 좀 더 효율적으로 사용한다\n",
    "- 비유 질문 같은 단어 추론 과제의 정확도가 높다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 단어 관계의 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 벡터를 2차원으로 시각화 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# 구글 뉴스 기사에 기초한 거대한 말뭉치로 훈련한 word2vec\n",
    "# 구글 뉴스 word2vec은 단어수가 300만이고 각 단어의 차원을 300이라 메모리를 줄이기 위해 20만개의 고빈도 용어를 적재\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format('C:/Users/today/NLP/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 빈도 살펴보기\n",
    "# vocab의 객체에서 주어진 키 값이 그 단어의 벡터가 아니라 단어 빈도와 단어 벡터 색인으로 이루어져있음\n",
    "\n",
    "import pandas as pd  # noqa\n",
    "vocab = pd.Series(wv.vocab)\n",
    "vocab.iloc[100000:100006]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
